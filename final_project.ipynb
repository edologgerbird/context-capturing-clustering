{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading in the Twitter dataset as described in the synopsis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/xizhao/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xizhao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xizhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', on_bad_lines='skip', names='target,user_id,date,flag,user,text'.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   target     user_id                          date      flag  \\\n0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n              user                                               text  \n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1    scotthamilton  is upset that he can't update his Facebook by ...  \n2         mattycus  @Kenichan I dived many times for the ball. Man...  \n3          ElleCTF    my whole body feels itchy and like its on fire   \n4           Karoli  @nationwideclass no, it's not behaving at all....  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>user_id</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    800000\n4    800000\nName: target, dtype: int64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 20,000 rows from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "tweets = raw.groupby('target').apply(lambda x: x.sample(10000, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    10000\n4    10000\nName: target, dtype: int64"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   target     user_id                          date      flag            user  \\\n0       0  1974671194  Sat May 30 13:36:31 PDT 2009  NO_QUERY         simba98   \n1       0  1997882236  Mon Jun 01 17:37:11 PDT 2009  NO_QUERY          Seve76   \n2       0  2177756662  Mon Jun 15 06:39:05 PDT 2009  NO_QUERY  x__claireyy__x   \n3       0  2216838047  Wed Jun 17 20:02:12 PDT 2009  NO_QUERY          Balasi   \n4       0  1880666283  Fri May 22 02:03:31 PDT 2009  NO_QUERY    djrickdawson   \n\n                                                text  \n0  @xnausikaax oh no! where did u order from? tha...  \n1  A great hard training weekend is over.  a coup...  \n2  Right, off to work  Only 5 hours to go until I...  \n3                    I am craving for japanese food   \n4  Jean Michel Jarre concert tomorrow  gotta work...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>user_id</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1974671194</td>\n      <td>Sat May 30 13:36:31 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>simba98</td>\n      <td>@xnausikaax oh no! where did u order from? tha...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1997882236</td>\n      <td>Mon Jun 01 17:37:11 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Seve76</td>\n      <td>A great hard training weekend is over.  a coup...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2177756662</td>\n      <td>Mon Jun 15 06:39:05 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>x__claireyy__x</td>\n      <td>Right, off to work  Only 5 hours to go until I...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2216838047</td>\n      <td>Wed Jun 17 20:02:12 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Balasi</td>\n      <td>I am craving for japanese food</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1880666283</td>\n      <td>Fri May 22 02:03:31 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>djrickdawson</td>\n      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a function that cleans the data. I will detail what each line of code does to make it clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this to each tweet\n",
    "def clean_data(tweet):\n",
    "    # removal of punctuations\n",
    "    tweet = re.sub(\"[^-9A-Za-z ]\", \"\" , tweet)\n",
    "    # convert to lowercase\n",
    "    tweet = \"\".join([i.lower() for i in tweet if i not in string.punctuation])\n",
    "    # tokenize the words temporarily\n",
    "    word_tokens = nltk.tokenize.word_tokenize(tweet)\n",
    "    # removal of non-alphabetical words\n",
    "    word_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "    # removal of non-english words like usernames\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    word_tokens = [w for w in word_tokens if w in words]\n",
    "    # removal of stop-words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    word_tokens = [stemmer.stem(w) for w in word_tokens]\n",
    "    # join back as string\n",
    "    tweet = \" \".join(word_tokens)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @xnausikaax oh no! where did u order from? that's horrible \n",
      "Processed: oh u order that horribl\n"
     ]
    }
   ],
   "source": [
    "# compare the tweet before and after processing\n",
    "print('Original:', tweets['text'][0])\n",
    "print('Processed:', clean_data(tweets['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(tweets.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/lx/ncb2g6ds77x94qt50qpgwq840000gn/T/ipykernel_9774/1693124340.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtweets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'cleaned_text'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtweets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclean_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtweets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtweets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtweets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4355\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfloat64\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4356\u001B[0m         \"\"\"\n\u001B[0;32m-> 4357\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mSeriesApply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconvert_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4358\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4359\u001B[0m     def _reduce(\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/apply.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1041\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_str\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1043\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1044\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1045\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0magg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/core/apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1099\u001B[0m                     \u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1100\u001B[0m                     \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0;31m# type: ignore[arg-type]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1101\u001B[0;31m                     \u001B[0mconvert\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_dtype\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1102\u001B[0m                 )\n\u001B[1;32m   1103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/_libs/lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/var/folders/lx/ncb2g6ds77x94qt50qpgwq840000gn/T/ipykernel_9774/2721346699.py\u001B[0m in \u001B[0;36mclean_data\u001B[0;34m(tweet)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mword_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mw\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mword_tokens\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mw\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misalpha\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;31m# removal of non-english words like usernames\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mwords\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mword_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mw\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mword_tokens\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mw\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mwords\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0;31m# removal of stop-words\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/corpus/reader/wordlist.py\u001B[0m in \u001B[0;36mwords\u001B[0;34m(self, fileids, ignore_lines_startswith)\u001B[0m\n\u001B[1;32m     19\u001B[0m         return [\n\u001B[1;32m     20\u001B[0m             \u001B[0mline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mline_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfileids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mignore_lines_startswith\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         ]\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/corpus/reader/wordlist.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     20\u001B[0m             \u001B[0mline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mline_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraw\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfileids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mignore_lines_startswith\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m         ]\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "tweets['cleaned_text'] = tweets['text'].apply(clean_data)\n",
    "tweets = tweets.drop(columns=['text'])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "708"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the sampled dataset with pre-processed tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(columns=['user_id', 'date', 'flag', 'user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any rows that have missing null values for cleaned text. \n",
    "This ensures that the pre-trained model has no issues processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['target'] = tweets['target'].apply(lambda x: 1 if x==4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain the phrasal embeddings for each cleaned tweet text in the tweets dataframe.\n",
    "We use the GloVe Twitter 25 model, which contains pre-trained GloVe vectors based on 2 billion tweets, 27 billion tokens, and 1.2 million vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "model = api.load('glove-twitter-25')\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a tweet and outputs a list of word embeddings for each word\n",
    "def get_embeddings(text):\n",
    "    return np.array([model[w] for w in text.split() if model.__contains__(w)])\n",
    "\n",
    "tweets['tokenized_text'] = tweets['cleaned_text'].apply(get_embeddings)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([tweets['tokenized_text'][0][0], tweets['tokenized_text'][0][0]]) - tweets['tokenized_text'][0][0]*2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Cluster Points and Centroids\n",
    "\n",
    "In this section, we will define the cluster points and centroids in an OOP format. The requirements of each object class are as follows:\n",
    "\n",
    "Class VectorSpace:\n",
    "\n",
    "        Attributes:\n",
    "        - Points: List(Points)\n",
    "        - Centroids: List(Centroids)\n",
    "        - Points Look Up table: Dict{String : Point}\n",
    "        - Centroid Look Up table: Dict{String : Centroid}\n",
    "\n",
    "        Methods:\n",
    "        - Injest Points -> List(Points)\n",
    "        - Inject Centroids -> List(Points)\n",
    "        - Tweets to Vector Space -> True\n",
    "        - Vader to Vector Space -> True\n",
    "        - Calculate Cosine Similarity between 2 Points -> Float\n",
    "        - Get most similar centroid to a point and the similarity score -> Centroid, Float\n",
    "        - Assign Static Clusters -> Boolean\n",
    "        - Assign Dynamic Clusters -> Boolean\n",
    "        - Translate Document -> List(Float)\n",
    "        - Plot Points and Centroids -> plt\n",
    "\n",
    "Class Point:\n",
    "\n",
    "        Attributes:\n",
    "        - Vector Location: Array(Float)\n",
    "        - Closest centroid: Point\n",
    "        - Similarity with closest centroid: Float\n",
    "        - Polarity: Int\n",
    "\n",
    "        Methods:\n",
    "        - Calculate Cosine Similarity with own centroid -> Float\n",
    "        - Set centroid -> Boolean\n",
    "        - Set polarity -> Boolean\n",
    "        - Set similarity -> Boolean\n",
    "\n",
    "Class Centroid(Point):\n",
    "\n",
    "        Attributes:\n",
    "        - Super.Point Attributes\n",
    "        - Points assigned: List(Points)\n",
    "        \n",
    "        Methods:\n",
    "        - Reset cluster: Boolean\n",
    "        - Add Point to cluster: Boolean\n",
    "        - Update vector location: Array(Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/lx/ncb2g6ds77x94qt50qpgwq840000gn/T/ipykernel_9774/2243069132.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'glove-twitter-25'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtweets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'tweets.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtweets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/downloader.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(name, return_path)\u001B[0m\n\u001B[1;32m    501\u001B[0m         \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minsert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBASE_DIR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    502\u001B[0m         \u001B[0mmodule\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m__import__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 503\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    504\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    505\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/gensim-data/glove-twitter-25/__init__.py\u001B[0m in \u001B[0;36mload_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mload_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbase_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'glove-twitter-25'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'glove-twitter-25.gz'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKeyedVectors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_word2vec_format\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mload_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[1;32m   1723\u001B[0m         return _load_word2vec_format(\n\u001B[1;32m   1724\u001B[0m             \u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfvocab\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbinary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0municode_errors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1725\u001B[0;31m             \u001B[0mlimit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatatype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdatatype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mno_header\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mno_header\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1726\u001B[0m         )\n\u001B[1;32m   1727\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m_load_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[1;32m   2071\u001B[0m             )\n\u001B[1;32m   2072\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2073\u001B[0;31m             \u001B[0m_word2vec_read_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfin\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcounts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvector_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatatype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2074\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mkv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvectors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2075\u001B[0m         logger.info(\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m_word2vec_read_text\u001B[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[1;32m   1976\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mline\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34mb''\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1977\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mEOFError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1978\u001B[0;31m         \u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_word2vec_line_to_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatatype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1979\u001B[0m         \u001B[0m_add_word_to_kv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcounts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1980\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m_word2vec_line_to_vector\u001B[0;34m(line, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[1;32m   1981\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1982\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_word2vec_line_to_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatatype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0municode_errors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1983\u001B[0;31m     \u001B[0mparts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_unicode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0municode_errors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1984\u001B[0m     \u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mdatatype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mparts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1985\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/utils.py\u001B[0m in \u001B[0;36many2unicode\u001B[0;34m(text, encoding, errors)\u001B[0m\n\u001B[1;32m    363\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 365\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    366\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = api.load('glove-twitter-25')\n",
    "tweets = pd.read_csv('tweets.csv', index_col=0)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    '''\n",
    "    A class used to represent each word in a document\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    word : str\n",
    "        The word represented by the point\n",
    "\n",
    "    vector : array\n",
    "        The vector representation of the word\n",
    "    \n",
    "    closest_centroid : Centroid\n",
    "        The closest centroid to the point\n",
    "\n",
    "    cosine_similarity_with_centroid : Centroid\n",
    "        The cosine similarity with the point's closest centroid\n",
    "\n",
    "    polarity : Float\n",
    "        The polarity of the point as determined by the polarity of its closest centroid\n",
    "\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    set_centroid(centroid)\n",
    "        Stores the centroid the point is assigned to\n",
    "\n",
    "    set_consine_similarity_with_centroid(similarity)\n",
    "        Sets the cosine similarity between the point and its closest centroid\n",
    "\n",
    "    set_polarity(polarity)\n",
    "        Sets the polarity of the point as determined by the polarity of its closest centroid\n",
    "\n",
    "    '''\n",
    "    def __init__(self, word, model):\n",
    "        self.word = word\n",
    "        self.vector = model[word] if model.__contains__(word) else None\n",
    "        self.closest_centroid = None\n",
    "        self.cosine_similarity_with_centroid = None\n",
    "        self.polarity = None\n",
    "\n",
    "    def set_centroid(self, centroid):\n",
    "        self.centroid = centroid\n",
    "        return True\n",
    "\n",
    "    def set_consine_similarity_with_centroid(self, similarity):\n",
    "        self.cosine_similarity_with_centroid = similarity\n",
    "        return True\n",
    "\n",
    "    def set_polarity(self, polarity):\n",
    "        self.polarity = polarity\n",
    "        return True\n",
    "\n",
    "    def get_cosine_similarity_with_centroid(self, mean_distance=False):\n",
    "        if self.closest_centroid is not None:\n",
    "            if not mean_distance:\n",
    "                return self.cosine_similarity_with_centroid\n",
    "            else:\n",
    "                return np.mean([point.cosine_similarity_with_centroid for point in self.closest_centroid.points])\n",
    "        else:\n",
    "            raise Exception('Unable to calculate cosine similarity with NoneType: Centroid is undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centroid(Point):\n",
    "    '''\n",
    "    A class used to represent each word in the lexical dictionary as a centroid. \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    word : str\n",
    "        The word represented by the point\n",
    "\n",
    "    vector : array\n",
    "        The vector representation of the word\n",
    "    \n",
    "    closest_centroid : Centroid\n",
    "        The closest centroid to the point\n",
    "\n",
    "    cosine_similarity_with_centroid : Centroid\n",
    "        The cosine similarity with the point's closest centroid\n",
    "\n",
    "    polarity : Float\n",
    "        The polarity of the point as determined by the polarity of its closest centroid\n",
    "\n",
    "    cluster_points : List(Points)\n",
    "        A list containing the Points assigned to the centroid as a cluster\n",
    "\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    resest_cluster()\n",
    "        resets the cluster to an empty list\n",
    "\n",
    "    add_point_to_cluster(point, similarity)\n",
    "        adds a Point to the centroid's cluster. Updates the Point's polarity, centroid and cosine similarity with that of and with the Centroid\n",
    "\n",
    "    update_vector_location()\n",
    "        For use in the dynamic clustering algorithm. Updates the vector location to the mean of all its points.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, word, polarity, model):\n",
    "        super.__init__(word, model)\n",
    "        self.polarity = polarity\n",
    "        self.cluster_points = []\n",
    "\n",
    "    def reset_cluster(self):\n",
    "        self.cluster_points = []\n",
    "        return True\n",
    "\n",
    "    def add_point_to_cluster(self, point, similarity):\n",
    "        self.cluster_points.append(point)\n",
    "        point.set_polarity(self.polarity)\n",
    "        point.set_centroid(self)\n",
    "        point.set_set_consine_similarity_with_centroid(similarity)\n",
    "        return True\n",
    "\n",
    "    def update_vector_location(self):\n",
    "        cluster_mean = sum([point.vector for point in self.cluster_points])/len(self.cluster_points)\n",
    "        self.vector = cluster_mean\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSpace:\n",
    "    '''\n",
    "    A class used to represent the Vector Space which will contain the points and centroids. \n",
    "    Separate VectorSpaces should be used for the Static and Dynamic Clustering approaches respectively.\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    points : List(Points)\n",
    "        The list of Points within the vector space.\n",
    "\n",
    "    centroids : List(Centroids)\n",
    "        The list of Centroids within the vector space.\n",
    "\n",
    "    points_look_up_table : Dict(Str:Point)\n",
    "        A look-up table to store the reference Points of each word. Allows for efficient translation of text to Point attributes\n",
    "    \n",
    "    centroids_look_up_table : Dict(Str:Point)\n",
    "        A look-up table to store the reference Centroid of each word. Allows for efficient translation of text to Centroid attributes\n",
    "\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    inject_point(point)\n",
    "        Injects a point into the vector space and updates the Points look-up table.\n",
    "\n",
    "    inject_centroid(centroid)\n",
    "        Injects a centroid into the vector space and updates the Centroids look-up table\n",
    "\n",
    "    tweet_to_vector_space(tweet, model)\n",
    "        Converts a tweet, aka document, of words into their respective points and injects the points into the vector space. \n",
    "        The model is the embedding model used to tokenise the text.\n",
    "        If the model does not contain the word, the Point is not created.\n",
    "\n",
    "    vader_to_vector_space(word, polarity, model)\n",
    "        Converts a word from the sentiment lexical dictionary into a Centroid and injects it into the vector space.\n",
    "        The model is the embedding model used to tokenise the text.\n",
    "        If the model does not contain the word, the Centroid is not created.\n",
    "\n",
    "    calculate_cosine_similarity(point1, point2)\n",
    "        Calculates the cosine similarity between 2 Points\n",
    "\n",
    "    get_most_similar_centroid(point)\n",
    "        Locates the closest Centroid to the Point from all the Centroids within the VectorSpace based on Cosine Similarity. \n",
    "        Returns the closest centroid and the Cosine Similarity between the point and the Centroid.\n",
    "\n",
    "    assign_static_clusters()\n",
    "        Executes the static clustering algorithm based on the paper \"Improvement of Sentiment Analysis based on clustring of Word2Vec\".\n",
    "\n",
    "    assign_dynamic_clusters()\n",
    "        Executes the proposed dynamic clustering algorithm.\n",
    "\n",
    "    translate_document(document, static=False, dynamic=False)\n",
    "        Translates a document of words into their respective similarity scores. \n",
    "        If a Centroid has negative polarity, the similarity will be transformed by a multiplicative factor of -1\n",
    "        If static, the mean similarity scores of all the points within a certain centroid will be returned for all these Points.\n",
    "        If dynamic, the raw similary will be returned\n",
    "\n",
    "    plot_points(n)\n",
    "        Plots the first n unique Points and Centroids on a 2-dimensional plane.\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "        self.centroids = []\n",
    "        self.points_look_up_table = {}\n",
    "        self.centroids_look_up_table = {}\n",
    "\n",
    "    def inject_point(self, point):\n",
    "        self.points.append(point)\n",
    "        self.points_look_up_table[point.word] = point\n",
    "        return True\n",
    "    \n",
    "    def inject_centroid(self, centroid):\n",
    "        self.centroids.append(centroid)\n",
    "        self.centroids_look_up_table[centroid.word] = centroid\n",
    "        return True\n",
    "\n",
    "    def tweet_to_vector_space(self, tweet, model):\n",
    "        for word in tweet.split():\n",
    "            if model.__contains__(word):\n",
    "                self.inject_point(Point(word, model))\n",
    "            else:\n",
    "                print(f\"INFO: Model does not contain {word}\")\n",
    "        return True\n",
    "\n",
    "    def vader_to_vector_space(self, word, polarity, model):\n",
    "        if model.__contains__(word):\n",
    "            self.inject_centroid(Centroid(word, polarity, model))\n",
    "        return True\n",
    "\n",
    "    def calculate_cosine_similarity(self, point1, point2):\n",
    "        return cosine_similarity(point1.vector, point2.vector)\n",
    "\n",
    "    def get_most_similar_centroid(self, point):\n",
    "        most_similar_centroid = None\n",
    "        max_similarity = 0\n",
    "\n",
    "        for centroid in self.centroids:\n",
    "            similarity = self.calculate_cosine_similarity(point, centroid)\n",
    "            if similarity > most_similar_centroid:\n",
    "                most_similar_centroid = centroid\n",
    "                max_similarity = similarity\n",
    "\n",
    "        return most_similar_centroid, max_similarity\n",
    "\n",
    "    def assign_static_clusters(self):\n",
    "        # Implement static cluster algorithm\n",
    "        for point in self.points:\n",
    "            most_similar_centroid, max_similarity = self.get_most_similar_centroid(point)\n",
    "            most_similar_centroid.add_point_to_cluster(point, max_similarity)\n",
    "        return True\n",
    "\n",
    "    def assign_dynamic_clusters(self):\n",
    "        # To do \n",
    "        # Implement dynamic cluster algorithm\n",
    "        return True\n",
    "\n",
    "    def translate_document(self, document, static=False, dynamic=False):\n",
    "        output = []\n",
    "        if static:\n",
    "            for word in document:\n",
    "                similarity_score = self.points_look_up_table[word].get_cosine_similarity_with_centroid(mean_distance=True)\n",
    "                if self.points_look_up_table[word].polarity < 0:\n",
    "                    similarity_score = -similarity_score\n",
    "                output.append(similarity_score)\n",
    "\n",
    "        elif dynamic:\n",
    "            for word in document:\n",
    "                similarity_score = self.points_look_up_table[word].get_cosine_similarity_with_centroid(mean_distance=False)\n",
    "                if self.points_look_up_table[word].polarity < 0:\n",
    "                    similarity_score = -similarity_score\n",
    "                output.append(similarity_score)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def plot_points(self, n):\n",
    "        words = []\n",
    "        vectors = []\n",
    "        for point in self.points:\n",
    "            if point.word not in words:\n",
    "                words.append(point.word)\n",
    "                vectors.append(point.vector)\n",
    "\n",
    "        words = np.asarray(words)[:n]\n",
    "        vectors = np.array(vectors)[:n]\n",
    "        \n",
    "        tsne = TSNE(n_components=2)\n",
    "        X_tsne = tsne.fit_transform(vectors)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "        for label, x, y in zip(words, X_tsne[:, 0], X_tsne[:, 1]):\n",
    "            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
    "        plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Injecting Tweets into Vector Space\n",
    "\n",
    "In this section, we will initialise a VectorSpace, and populate it with words from the Tweets. At the same time, we will maintain a hashtable to keep track of the corresponding point for each word that we can translate the points back to words in the form of the respective maximum similarity value in O(1) time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space = VectorSpace()\n",
    "tweets['cleaned_text'].apply(lambda x: vector_space.tweet_to_vector_space(x, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space.plot_points(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### vader lexicons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# obtaining Vader Sentiment lexicons\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "lexicons = analyzer.lexicon  # total: 7506"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4880\n"
     ]
    }
   ],
   "source": [
    "# filter lexicons to keep those that also appear in vocab\n",
    "vocab = model.index_to_key\n",
    "lexicons_filtered = {k: v for k, v in lexicons.items() if k in vocab}\n",
    "print(len(lexicons_filtered))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f9a0a7ae4c556cc9d70b63c8d6e054a1e46c823d790d1334b23650b163d6b82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
