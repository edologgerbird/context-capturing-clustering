{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading in the Twitter dataset as described in the synopsis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\edmun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', on_bad_lines='skip', names='target,user_id,date,flag,user,text'.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target     user_id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "4    800000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 20,000 rows from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "tweets = raw.groupby('target').apply(lambda x: x.sample(10000, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "4    10000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1974671194</td>\n",
       "      <td>Sat May 30 13:36:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>simba98</td>\n",
       "      <td>@xnausikaax oh no! where did u order from? tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1997882236</td>\n",
       "      <td>Mon Jun 01 17:37:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Seve76</td>\n",
       "      <td>A great hard training weekend is over.  a coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2177756662</td>\n",
       "      <td>Mon Jun 15 06:39:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>x__claireyy__x</td>\n",
       "      <td>Right, off to work  Only 5 hours to go until I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2216838047</td>\n",
       "      <td>Wed Jun 17 20:02:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Balasi</td>\n",
       "      <td>I am craving for japanese food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1880666283</td>\n",
       "      <td>Fri May 22 02:03:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>djrickdawson</td>\n",
       "      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target     user_id                          date      flag            user  \\\n",
       "0       0  1974671194  Sat May 30 13:36:31 PDT 2009  NO_QUERY         simba98   \n",
       "1       0  1997882236  Mon Jun 01 17:37:11 PDT 2009  NO_QUERY          Seve76   \n",
       "2       0  2177756662  Mon Jun 15 06:39:05 PDT 2009  NO_QUERY  x__claireyy__x   \n",
       "3       0  2216838047  Wed Jun 17 20:02:12 PDT 2009  NO_QUERY          Balasi   \n",
       "4       0  1880666283  Fri May 22 02:03:31 PDT 2009  NO_QUERY    djrickdawson   \n",
       "\n",
       "                                                text  \n",
       "0  @xnausikaax oh no! where did u order from? tha...  \n",
       "1  A great hard training weekend is over.  a coup...  \n",
       "2  Right, off to work  Only 5 hours to go until I...  \n",
       "3                    I am craving for japanese food   \n",
       "4  Jean Michel Jarre concert tomorrow  gotta work...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a function that cleans the data. I will detail what each line of code does to make it clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this to each tweet\n",
    "def clean_data(tweet):\n",
    "    # removal of punctuations\n",
    "    tweet = re.sub(\"[^-9A-Za-z ]\", \"\" , tweet)\n",
    "    # convert to lowercase\n",
    "    tweet = \"\".join([i.lower() for i in tweet if i not in string.punctuation])\n",
    "    # tokenize the words temporarily\n",
    "    word_tokens = nltk.tokenize.word_tokenize(tweet)\n",
    "    # removal of non-alphabetical words\n",
    "    word_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "    # removal of non-english words like usernames\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    word_tokens = [w for w in word_tokens if w in words]\n",
    "    # removal of stop-words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    # # stemming\n",
    "    # stemmer = nltk.stem.PorterStemmer()\n",
    "    # word_tokens = [stemmer.stem(w) for w in word_tokens]\n",
    "    # join back as string\n",
    "    tweet = \" \".join(word_tokens)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @xnausikaax oh no! where did u order from? that's horrible \n",
      "Processed: oh u order that horribl\n"
     ]
    }
   ],
   "source": [
    "# compare the tweet before and after processing\n",
    "print('Original:', tweets['text'][0])\n",
    "print('Processed:', clean_data(tweets['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaned_text'] = tweets['text'].apply(clean_data)\n",
    "tweets = tweets.drop(columns=['text'])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the sampled dataset with pre-processed tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(columns=['user_id', 'date', 'flag', 'user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh u order that horribl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>great hard train weekend coupl day rest lot co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>right work go free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>crave food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>jean concert tomorrow got ta work though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                       cleaned_text\n",
       "0       0                            oh u order that horribl\n",
       "1       0  great hard train weekend coupl day rest lot co...\n",
       "2       0                                 right work go free\n",
       "3       0                                         crave food\n",
       "4       0           jean concert tomorrow got ta work though"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any rows that have missing null values for cleaned text. \n",
    "This ensures that the pre-trained model has no issues processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['target'] = tweets['target'].apply(lambda x: 1 if x==4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain the phrasal embeddings for each cleaned tweet text in the tweets dataframe.\n",
    "We use the GloVe Twitter 25 model, which contains pre-trained GloVe vectors based on 2 billion tweets, 27 billion tokens, and 1.2 million vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh u order that horribl</td>\n",
       "      <td>[[0.34172, -0.17305, 0.23311, 0.057375, -0.761...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>great hard train weekend coupl day rest lot co...</td>\n",
       "      <td>[[-0.84229, 0.36512, -0.38841, -0.46118, 0.243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>right work go free</td>\n",
       "      <td>[[-0.43876, 0.095692, 0.0030075, -0.13195, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>crave food</td>\n",
       "      <td>[[-0.85832, 0.48558, -0.61313, 0.72877, -0.656...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>jean concert tomorrow got ta work though</td>\n",
       "      <td>[[-0.70756, -1.2247, 0.087766, 0.27264, -0.412...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                       cleaned_text  \\\n",
       "0       0                            oh u order that horribl   \n",
       "1       0  great hard train weekend coupl day rest lot co...   \n",
       "2       0                                 right work go free   \n",
       "3       0                                         crave food   \n",
       "4       0           jean concert tomorrow got ta work though   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [[0.34172, -0.17305, 0.23311, 0.057375, -0.761...  \n",
       "1  [[-0.84229, 0.36512, -0.38841, -0.46118, 0.243...  \n",
       "2  [[-0.43876, 0.095692, 0.0030075, -0.13195, -0....  \n",
       "3  [[-0.85832, 0.48558, -0.61313, 0.72877, -0.656...  \n",
       "4  [[-0.70756, -1.2247, 0.087766, 0.27264, -0.412...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes in a tweet and outputs a list of word embeddings for each word\n",
    "def get_embeddings(text):\n",
    "    return np.array([model[w] for w in text.split() if model.__contains__(w)])\n",
    "\n",
    "tweets['tokenized_text'] = tweets['cleaned_text'].apply(get_embeddings)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([tweets['tokenized_text'][0][0], tweets['tokenized_text'][0][0]]) - tweets['tokenized_text'][0][0]*2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Cluster Points and Centroids\n",
    "\n",
    "In this section, we will define the cluster points and centroids in an OOP format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\edmun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gc\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import gensim.downloader as gensim_api\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh u order thats horrible</td>\n",
       "      <td>[[ 3.4172e-01 -1.7305e-01  2.3311e-01  5.7375e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>great hard training weekend couple days rest l...</td>\n",
       "      <td>[[-8.4229e-01  3.6512e-01 -3.8841e-01 -4.6118e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>right work go free</td>\n",
       "      <td>[[-4.3876e-01  9.5692e-02  3.0075e-03 -1.3195e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>craving food</td>\n",
       "      <td>[[-1.2275    0.61425   0.23204   0.4787   -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>jean concert tomorrow got ta work though</td>\n",
       "      <td>[[-7.0756e-01 -1.2247e+00  8.7766e-02  2.7264e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1</td>\n",
       "      <td>arms morning baby face boy guitar hero</td>\n",
       "      <td>[[-1.4197   -0.59392   0.034198  0.7004    0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "      <td>sweet mother extend royal welcome</td>\n",
       "      <td>[[-1.0864   -0.61674   0.33613   0.43232  -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>1</td>\n",
       "      <td>wouldnt mind watching derby mint julep</td>\n",
       "      <td>[[ 5.7749e-02  1.3264e+00 -1.6871e-02 -5.5858e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1</td>\n",
       "      <td>shiny nice like shiny hate vista mean passion</td>\n",
       "      <td>[[-9.4931e-01 -5.5718e-01  1.3540e-01 -1.2416e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "      <td>lake outside</td>\n",
       "      <td>[[-2.4857   -0.82327   0.31095  -0.42148  -1.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19479 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                       cleaned_text  \\\n",
       "0           0                          oh u order thats horrible   \n",
       "1           0  great hard training weekend couple days rest l...   \n",
       "2           0                                 right work go free   \n",
       "3           0                                       craving food   \n",
       "4           0           jean concert tomorrow got ta work though   \n",
       "...       ...                                                ...   \n",
       "19995       1             arms morning baby face boy guitar hero   \n",
       "19996       1                  sweet mother extend royal welcome   \n",
       "19997       1             wouldnt mind watching derby mint julep   \n",
       "19998       1      shiny nice like shiny hate vista mean passion   \n",
       "19999       1                                       lake outside   \n",
       "\n",
       "                                          tokenized_text  \n",
       "0      [[ 3.4172e-01 -1.7305e-01  2.3311e-01  5.7375e...  \n",
       "1      [[-8.4229e-01  3.6512e-01 -3.8841e-01 -4.6118e...  \n",
       "2      [[-4.3876e-01  9.5692e-02  3.0075e-03 -1.3195e...  \n",
       "3      [[-1.2275    0.61425   0.23204   0.4787   -0.5...  \n",
       "4      [[-7.0756e-01 -1.2247e+00  8.7766e-02  2.7264e...  \n",
       "...                                                  ...  \n",
       "19995  [[-1.4197   -0.59392   0.034198  0.7004    0.1...  \n",
       "19996  [[-1.0864   -0.61674   0.33613   0.43232  -0.2...  \n",
       "19997  [[ 5.7749e-02  1.3264e+00 -1.6871e-02 -5.5858e...  \n",
       "19998  [[-9.4931e-01 -5.5718e-01  1.3540e-01 -1.2416e...  \n",
       "19999  [[-2.4857   -0.82327   0.31095  -0.42148  -1.6...  \n",
       "\n",
       "[19479 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = api.load('glove-twitter-25')\n",
    "tweets = pd.read_csv('tweets.csv', index_col=0)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Training, Validation and Testing data\n",
    "\n",
    "In this section, we will be splitting the tweets into training, validation and testing sets. The proportion will be as follows:\n",
    "1. Training: 0.70\n",
    "2. Validating: 0.15\n",
    "3. Testing: 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.70\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "MAX_TWEET_LENGTH = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9213</th>\n",
       "      <td>0</td>\n",
       "      <td>yolk best bit</td>\n",
       "      <td>[[-1.7264e-01 -3.6231e-01  3.6885e-01  8.2955e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18833</th>\n",
       "      <td>1</td>\n",
       "      <td>fist one u bet second one yep fun</td>\n",
       "      <td>[[-9.4536e-01  4.2714e-01  2.9669e-01  9.2002e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>0</td>\n",
       "      <td>lot hungry u</td>\n",
       "      <td>[[ 1.6807e-01  4.8304e-01  3.1292e-02  3.9010e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>0</td>\n",
       "      <td>neck</td>\n",
       "      <td>[[-1.2556e+00 -5.3265e-01  1.5854e-01  8.2836e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>0</td>\n",
       "      <td>pool goddess us lost next match wont going bet...</td>\n",
       "      <td>[[-1.5123e+00  2.5732e-01  7.6255e-01 -4.6525e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042</th>\n",
       "      <td>0</td>\n",
       "      <td>already hurting work</td>\n",
       "      <td>[[-2.6674e-01  9.5784e-01  8.5408e-01 -1.2863e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>0</td>\n",
       "      <td>signal academy papa roach escape pit moment</td>\n",
       "      <td>[[ 0.38179    0.24277   -0.4581    -0.41338   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15848</th>\n",
       "      <td>1</td>\n",
       "      <td>sat bed gone go beach</td>\n",
       "      <td>[[-6.8048e-01  6.9749e-01  1.1411e+00 -9.0814e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18580</th>\n",
       "      <td>1</td>\n",
       "      <td>great night dinner ash long wait food worth</td>\n",
       "      <td>[[-8.4229e-01  3.6512e-01 -3.8841e-01 -4.6118e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14801</th>\n",
       "      <td>1</td>\n",
       "      <td>hope marathon went well see next weekend</td>\n",
       "      <td>[[-0.76599    0.80836    0.015636  -0.22972   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                       cleaned_text  \\\n",
       "9213        0                                      yolk best bit   \n",
       "18833       1                  fist one u bet second one yep fun   \n",
       "2357        0                                       lot hungry u   \n",
       "3696        0                                               neck   \n",
       "2889        0  pool goddess us lost next match wont going bet...   \n",
       "...       ...                                                ...   \n",
       "5042        0                               already hurting work   \n",
       "5346        0        signal academy papa roach escape pit moment   \n",
       "15848       1                              sat bed gone go beach   \n",
       "18580       1        great night dinner ash long wait food worth   \n",
       "14801       1           hope marathon went well see next weekend   \n",
       "\n",
       "                                          tokenized_text  \n",
       "9213   [[-1.7264e-01 -3.6231e-01  3.6885e-01  8.2955e...  \n",
       "18833  [[-9.4536e-01  4.2714e-01  2.9669e-01  9.2002e...  \n",
       "2357   [[ 1.6807e-01  4.8304e-01  3.1292e-02  3.9010e...  \n",
       "3696   [[-1.2556e+00 -5.3265e-01  1.5854e-01  8.2836e...  \n",
       "2889   [[-1.5123e+00  2.5732e-01  7.6255e-01 -4.6525e...  \n",
       "...                                                  ...  \n",
       "5042   [[-2.6674e-01  9.5784e-01  8.5408e-01 -1.2863e...  \n",
       "5346   [[ 0.38179    0.24277   -0.4581    -0.41338   ...  \n",
       "15848  [[-6.8048e-01  6.9749e-01  1.1411e+00 -9.0814e...  \n",
       "18580  [[-8.4229e-01  3.6512e-01 -3.8841e-01 -4.6118e...  \n",
       "14801  [[-0.76599    0.80836    0.015636  -0.22972   ...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_sampled = tweets.sample(10000, random_state=RANDOM_STATE)\n",
    "tweets_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_sampled['cleaned_text']\n",
    "y = tweets_sampled.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 1400 \n",
      "X_val: 300 \n",
      "X_test: 300 \n",
      "y_train: 1400 \n",
      "y_val: 300 \n",
      "y_test: 300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, train_size=TRAIN_SIZE, random_state=RANDOM_STATE, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=(TEST_SIZE)/(TEST_SIZE+VAL_SIZE), random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "print(\n",
    "    f'X_train: {X_train.shape[0]} \\n'\n",
    "    f'X_val: {X_val.shape[0]} \\n'\n",
    "    f'X_test: {X_test.shape[0]} \\n'\n",
    "    f'y_train: {y_train.shape[0]} \\n'\n",
    "    f'y_val: {y_val.shape[0]} \\n'\n",
    "    f'y_test: {y_test.shape[0]} \\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the Cluster Points and Centroids\n",
    "\n",
    "In this section, we will define the cluster points and centroids in an OOP format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    '''\n",
    "    A class used to represent each word in a document\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    word : str\n",
    "        The word represented by the point\n",
    "\n",
    "    vector : array\n",
    "        The vector representation of the word\n",
    "    \n",
    "    closest_centroid : Centroid\n",
    "        The closest centroid to the point\n",
    "\n",
    "    cosine_similarity_with_centroid : Centroid\n",
    "        The cosine similarity with the point's closest centroid\n",
    "\n",
    "    polarity : Float\n",
    "        The polarity of the point as determined by the polarity of its closest centroid\n",
    "\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    reset_point()\n",
    "        resets the Point's closest centroid, similarity score and polarity to None\n",
    "\n",
    "    set_centroid(centroid)\n",
    "        Stores the centroid the point is assigned to\n",
    "\n",
    "    set_cosine_similarity_with_centroid(similarity)\n",
    "        Sets the cosine similarity between the point and its closest centroid\n",
    "\n",
    "    set_polarity(polarity)\n",
    "        Sets the polarity of the point as determined by the polarity of its closest centroid\n",
    "\n",
    "    get_cosine_similarity_with_centroid(mean_distance=False)\n",
    "        Returns the cosine similarity\n",
    "\n",
    "    '''\n",
    "    def __init__(self, word, model):\n",
    "        self.word = word \n",
    "        self.vector = model[word] if model.__contains__(word) else None\n",
    "        self.closest_centroid = None\n",
    "        self.cosine_similarity_with_centroid = None\n",
    "        self.polarity = None\n",
    "\n",
    "    def reset_point(self):\n",
    "        self.closest_centroid = None\n",
    "        self.cosine_similarity_with_centroid = None\n",
    "        self.polarity = None\n",
    "        return True\n",
    "\n",
    "    def set_centroid(self, centroid):\n",
    "        self.closest_centroid = centroid\n",
    "        return True\n",
    "\n",
    "    def set_cosine_similarity_with_centroid(self, similarity):\n",
    "        self.cosine_similarity_with_centroid = similarity\n",
    "        return True\n",
    "\n",
    "    def set_polarity(self, polarity):\n",
    "        self.polarity = polarity\n",
    "        return True\n",
    "\n",
    "    def get_cosine_similarity_with_centroid(self, mean_distance=False):\n",
    "        if self.closest_centroid is not None:\n",
    "            if not mean_distance:\n",
    "                return self.cosine_similarity_with_centroid\n",
    "            else:\n",
    "                return np.mean([point.cosine_similarity_with_centroid for point in self.closest_centroid.cluster_points])\n",
    "        else:\n",
    "            raise Exception('Unable to calculate cosine similarity with NoneType: Centroid is undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centroid(Point):\n",
    "    '''\n",
    "    A class used to represent each word in the lexical dictionary as a centroid. \n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    word : str\n",
    "        The word represented by the point\n",
    "\n",
    "    vector : array\n",
    "        The vector representation of the word\n",
    "    \n",
    "    closest_centroid : Centroid\n",
    "        The closest centroid to the point\n",
    "\n",
    "    cosine_similarity_with_centroid : Centroid\n",
    "        The cosine similarity with the point's closest centroid\n",
    "\n",
    "    polarity : Float\n",
    "        The polarity of the point as determined by the polarity of its closest centroid\n",
    "\n",
    "    cluster_points : List(Points)\n",
    "        A list containing the Points assigned to the centroid as a cluster\n",
    "\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    resest_cluster()\n",
    "        Resets all the cluster's points and resets the cluster to an empty list.\n",
    "\n",
    "    add_point_to_cluster(point, similarity)\n",
    "        Adds a Point to the centroid's cluster. Updates the Point's polarity, centroid and cosine similarity with that of and with the Centroid\n",
    "\n",
    "    update_vector_location()\n",
    "        For use in the dynamic clustering algorithm. Updates the vector location to the mean of all its points.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, word, polarity, model):\n",
    "        super().__init__(word, model)\n",
    "        self.polarity = polarity\n",
    "        self.cluster_points = []\n",
    "\n",
    "    def reset_cluster(self):\n",
    "        for point in self.cluster_points:\n",
    "            point.reset_point()\n",
    "        self.cluster_points = []\n",
    "        return True\n",
    "\n",
    "    def add_point_to_cluster(self, point, similarity):\n",
    "        self.cluster_points.append(point)\n",
    "        point.set_polarity(self.polarity)\n",
    "        point.set_centroid(self)\n",
    "        point.set_cosine_similarity_with_centroid(similarity)\n",
    "        return True\n",
    "\n",
    "    def remove_point_from_cluster(self, point):\n",
    "        self.cluster_points.remove(point)\n",
    "        point.reset_point()\n",
    "        return True\n",
    "\n",
    "    def update_vector_location(self):\n",
    "        cluster_mean = sum([point.vector for point in self.cluster_points])/len(self.cluster_points)\n",
    "        self.vector = cluster_mean\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSpace:\n",
    "    '''\n",
    "    A class used to represent the Vector Space which will contain the points and centroids. \n",
    "    Separate VectorSpaces should be used for the Static and Dynamic Clustering approaches respectively.\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    points : List(Points)\n",
    "        The list of Points within the vector space.\n",
    "\n",
    "    centroids : List(Centroids)\n",
    "        The list of Centroids within the vector space.\n",
    "\n",
    "    points_look_up_table : Dict(Str:Point)\n",
    "        A look-up table to store the reference Points of each word. Allows for efficient translation of text to Point attributes\n",
    "    \n",
    "    centroids_look_up_table : Dict(Str:Point)\n",
    "        A look-up table to store the reference Centroid of each word. Allows for efficient translation of text to Centroid attributes\n",
    "\n",
    "    word2vec_model : Model\n",
    "        The word embedding model used to embed words into the VectorSpace. ie. GloVe-Twitter-25\n",
    "\n",
    "    lexicon : Dict(Str:Float)\n",
    "        The sentiment lexical dictionary used for the VectorSpace. \n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    inject_point(point)\n",
    "        Injects a point into the vector space and updates the Points look-up table.\n",
    "\n",
    "    inject_centroid(centroid)\n",
    "        Injects a centroid into the vector space and updates the Centroids look-up table\n",
    "\n",
    "    tweet_to_vector_space(tweet, model)\n",
    "        Converts a tweet, aka document, of words into their respective points and injects the points into the vector space. \n",
    "        The model is the embedding model used to tokenise the text.\n",
    "        If the model does not contain the word, the Point is not created.\n",
    "\n",
    "    load_lexicon(lexical analyser)\n",
    "        Loads the lexical dictionary from the lexical analyser and stores within the VectorSpace object.\n",
    "\n",
    "    lexicon_to_vector_space(word, polarity, model)\n",
    "        Converts a word from the sentiment lexical dictionary into a Centroid and injects it into the vector space.\n",
    "        The model is the embedding model used to tokenise the text.\n",
    "        If the model does not contain the word, the Centroid is not created.\n",
    "\n",
    "    init_centroids()\n",
    "        Initialises centroids into the VectorSpace based in the Lexical Dictionary\n",
    "\n",
    "    calculate_cosine_similarity(point1, point2)\n",
    "        Calculates the cosine similarity between 2 Points. Utilises Numpy matrix operations for increased efficiency.\n",
    "\n",
    "    get_most_similar_centroid(point)\n",
    "        Locates the closest Centroid to the Point from all the Centroids within the VectorSpace based on Cosine Similarity. \n",
    "        Returns the closest centroid and the Cosine Similarity between the point and the Centroid.\n",
    "\n",
    "    assign_static_clusters()\n",
    "        Executes the static clustering algorithm based on the paper \"Improvement of Sentiment Analysis based on clustring of Word2Vec\".\n",
    "\n",
    "    assign_dynamic_clusters()\n",
    "        Executes the proposed dynamic clustering algorithm.\n",
    "\n",
    "    get_similarity_score_of_unseen_word(word)\n",
    "        For an unseen word in the testing data, return its closest centroid and cosine similarity score.\n",
    "\n",
    "    translate_text_to_similarity(document, static=False, dynamic=False)\n",
    "        Translates a document of words into their respective similarity scores. \n",
    "        If an unseen word is encountered from testing data, get_similarity_score_of_unseen_word will be executed.\n",
    "        If a Centroid has negative polarity, the similarity will be transformed by a multiplicative factor of -1\n",
    "        If static, the mean similarity scores of all the points within a certain centroid will be returned for all these Points.\n",
    "        If dynamic, the raw similary will be returned\n",
    "\n",
    "    fit(Series, static=False, dynamic=False)\n",
    "        Main client facing function. This operation will train the VectorSpace with the series of tweets.\n",
    "\n",
    "    transform(Series, static=False, dynamic=False)\n",
    "        Transforms words within a series of String to their respective similarity scores. \n",
    "\n",
    "    plot_points(n)\n",
    "        Plots the first n unique Points and Centroids on a 2-dimensional plane.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, word2vec_model, lexicon_dictionary):\n",
    "        self.points = []\n",
    "        self.centroids = []\n",
    "        self.points_look_up_table = {}\n",
    "        self.centroids_look_up_table = {}\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.load_lexicon(lexicon_dictionary)\n",
    "        self.init_centroids()\n",
    "        print(f\"SUCCESS: VectorSpace initialised with {len(self.centroids)} centroids\")\n",
    "\n",
    "    def inject_point(self, point):\n",
    "        self.points.append(point)\n",
    "        self.points_look_up_table[point.word] = point\n",
    "        return True\n",
    "    \n",
    "    def inject_centroid(self, centroid):\n",
    "        self.centroids.append(centroid)\n",
    "        self.centroids_look_up_table[centroid.word] = centroid\n",
    "        return True\n",
    "\n",
    "    def tweet_to_vector_space(self, tweet, word2vec_model, verbose=False):\n",
    "        for word in tweet.split():\n",
    "            if word2vec_model.__contains__(word):\n",
    "                self.inject_point(Point(word, self.word2vec_model))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"INFO: Model does not contain {word}\")\n",
    "        return True\n",
    "\n",
    "    def load_lexicon(self, lexicon_dictionary):\n",
    "        self.lexicons = lexicon_dictionary.lexicon\n",
    "        return True\n",
    "\n",
    "    def lexicon_to_vector_space(self, word, polarity, model):\n",
    "        if model.__contains__(word):\n",
    "            self.inject_centroid(Centroid(word, polarity, model))\n",
    "        return True\n",
    "\n",
    "    def init_centroids(self):\n",
    "        for word, polarity in self.lexicons.items():\n",
    "            self.lexicon_to_vector_space(word, polarity, self.word2vec_model)\n",
    "        return True\n",
    "\n",
    "    def calculate_cosine_similarity(self, point1, point2):\n",
    "        return np.dot(point1.vector, point2.vector)/(np.linalg.norm(point1.vector)* np.linalg.norm(point2.vector))\n",
    "\n",
    "    def get_most_similar_centroid(self, point):\n",
    "        most_similar_centroid = None\n",
    "        max_similarity = 0\n",
    "\n",
    "        for centroid in self.centroids:\n",
    "            similarity = self.calculate_cosine_similarity(point, centroid)\n",
    "            if similarity > max_similarity:\n",
    "                most_similar_centroid = centroid\n",
    "                max_similarity = similarity\n",
    "\n",
    "        return most_similar_centroid, max_similarity\n",
    "\n",
    "    def assign_static_clusters(self):\n",
    "        # Implement static cluster algorithm\n",
    "        for point in tqdm(self.points, desc=\"Assigning Points to Clusters...\"):\n",
    "            most_similar_centroid, max_similarity = self.get_most_similar_centroid(point)\n",
    "            most_similar_centroid.add_point_to_cluster(point, max_similarity)\n",
    "        return True\n",
    "\n",
    "    def assign_dynamic_clusters(self, max_iterations = 20):\n",
    "        # Implement dynamic cluster algorithm\n",
    "        # not a single point changes cluster in 1 iteration, or can try setting a max iteration\n",
    "        centroid_change_flag = True\n",
    "        counter = 1\n",
    "        self.assign_static_clusters()\n",
    "        while centroid_change_flag and counter <= max_iterations:\n",
    "            print(f\"INFO: Iteration {counter}\")\n",
    "            centroid_change_flag = False\n",
    "            \n",
    "            for centroid in tqdm(self.centroids, desc=\"Updating Centroid Locations\"):\n",
    "                if len(centroid.cluster_points) > 0:\n",
    "                    centroid.update_vector_location()\n",
    "            for point in tqdm(self.points, desc=\"Reassigning Points\"):\n",
    "                most_similar_centroid, max_similarity = self.get_most_similar_centroid(point)\n",
    "                if most_similar_centroid is not point.closest_centroid:\n",
    "                    # print(\"reassigned\")\n",
    "                    centroid_change_flag = True\n",
    "                    point.closest_centroid.remove_point_from_cluster(point)\n",
    "                    most_similar_centroid.add_point_to_cluster(point, max_similarity)\n",
    "            counter += 1\n",
    "        return True\n",
    "\n",
    "    def get_similarity_score_of_unseen_word(self, word):\n",
    "        most_similar_centroid, max_similarity = self.get_most_similar_centroid(Point(word, self.word2vec_model))\n",
    "        # Consider mean values for centroids with points inside\n",
    "        return most_similar_centroid, max_similarity\n",
    "\n",
    "    def translate_text_to_similarity(self, document, static=False, dynamic=False):\n",
    "        output = []\n",
    "        for word in document.split():\n",
    "            if self.word2vec_model.__contains__(word):\n",
    "                if word not in self.points_look_up_table: # unseen test vocabulary\n",
    "                    most_similar_centroid, similarity_score = self.get_similarity_score_of_unseen_word(word)\n",
    "                    # similarity_score *= most_similar_centroid.polarity\n",
    "                    if most_similar_centroid.polarity < 0:\n",
    "                        similarity_score = -1 * similarity_score\n",
    "                else:\n",
    "                    similarity_score = self.points_look_up_table[word].get_cosine_similarity_with_centroid(mean_distance=static)\n",
    "                    # similarity_score *= self.points_look_up_table[word].polarity\n",
    "                    if self.points_look_up_table[word].polarity < 0:\n",
    "                        similarity_score =  -1 * similarity_score\n",
    "                output.append(similarity_score)\n",
    "        output += [0] * (MAX_TWEET_LENGTH - len(output))         \n",
    "\n",
    "        return np.array(output)\n",
    "\n",
    "    def fit(self, X_train, static=False, dynamic=False, max_iterations = 20):\n",
    "        if not static and not dynamic:\n",
    "            raise Exception(\"Please specify static or dynamic fitting\")\n",
    "        \n",
    "        X_train.apply(lambda x: self.tweet_to_vector_space(x, word2vec_model=self.word2vec_model))\n",
    "       \n",
    "        if static:\n",
    "            self.assign_static_clusters()\n",
    "        elif dynamic:\n",
    "            self.assign_dynamic_clusters(max_iterations = 20)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def transform(self, X, static=False, dynamic=False):\n",
    "        if not static and not dynamic:\n",
    "            raise Exception(\"Please specify static or dynamic transformation\")\n",
    "        X_transformed = X.apply(lambda x: self.translate_text_to_similarity(x, static=static, dynamic=dynamic))\n",
    "        return np.array(X_transformed.tolist())\n",
    "\n",
    "\n",
    "    def plot_points(self, n):\n",
    "        words = []\n",
    "        vectors = []\n",
    "        for point in self.points:\n",
    "            if point.word not in words:\n",
    "                words.append(point.word)\n",
    "                vectors.append(point.vector)\n",
    "\n",
    "        words = np.asarray(words)[:n]\n",
    "        vectors = np.array(vectors)[:n]\n",
    "        centroids = np.array([c.vector for c in self.centroids])\n",
    "        \n",
    "        tsne = TSNE(n_components=2)\n",
    "        X_tsne = tsne.fit_transform(vectors)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "        for label, x, y in zip(words, X_tsne[:, 0], X_tsne[:, 1]):\n",
    "            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
    "        plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Static Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Injecting Training Set into Vector Space\n",
    "\n",
    "In this section, we will initialise a VectorSpace, and populate it with words from the training Tweets. At the same time, we will maintain a hashtable to keep track of the corresponding point for each word that we can translate the points back to words in the form of the respective maximum similarity value in O(1) time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5545               seriously shot knee would fail physical\n",
       "4493                                               message\n",
       "16308                                        time everyone\n",
       "16241                                                  new\n",
       "8783     season found last five stop watching lost comp...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "word2vec_model = gensim_api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising and Training Static and Dynamic VectorSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: VectorSpace initialised with 4880 centroids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Points to Clusters...: 100%|██████████| 7390/7390 [04:10<00:00, 29.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_space_static = VectorSpace(word2vec_model, analyzer)\n",
    "vector_space_static.fit(X_train, static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: VectorSpace initialised with 4880 centroids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Points to Clusters...: 100%|██████████| 7390/7390 [04:07<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 484902.12it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:14<00:00, 29.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 462924.43it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:08<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 465440.32it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:06<00:00, 29.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 481321.66it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:06<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 487756.26it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:06<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 467705.68it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:11<00:00, 29.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Centroid Locations: 100%|██████████| 4880/4880 [00:00<00:00, 498567.83it/s]\n",
      "Reassigning Points: 100%|██████████| 7390/7390 [04:21<00:00, 28.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_space_dynamic = VectorSpace(word2vec_model, analyzer)\n",
    "vector_space_dynamic.fit(X_train, dynamic=True, max_iterations = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle trained vector spaces\n",
    "\n",
    "import pickle\n",
    "\n",
    "container = {\n",
    "    \"static_VectorSpace\": vector_space_static,\n",
    "    \"dynamic_VectorSpace\": vector_space_dynamic,\n",
    "    \"lexical_dictionary\": analyzer,\n",
    "    \"word2vec_model\": word2vec_model,\n",
    "}\n",
    "\n",
    "with open('container.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(container, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('container.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    container = pickle.load(f)\n",
    "\n",
    "vector_space_static = container[\"static_VectorSpace\"]\n",
    "vector_space_dynamic = container[\"dynamic_VectorSpace\"]\n",
    "analyzer = container[\"lexical_dictionary\"]\n",
    "word2vec_model = container[\"word2vec_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_static_transformed = vector_space_static.transform(X_train, static=True)\n",
    "X_test_static_transformed = vector_space_static.transform(X_test, static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dynamic_transformed = vector_space_dynamic.transform(X_train, dynamic=True)\n",
    "X_test_dynamic_transformed = vector_space_dynamic.transform(X_test, dynamic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training \n",
    "\n",
    "In this section, we will train two supervised models: PassiveAggressive Classifier and a baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(precision, recall):\n",
    "    if precision == recall == 0:\n",
    "        return 0\n",
    "    return 2* (precision*recall)/(precision+recall)\n",
    "\n",
    "def model_pipeline(model, X_train_transformed, y_train, X_test_transformed, y_test):\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    pred = model.predict(X_test_transformed)\n",
    "\n",
    "    loss = log_loss(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    \n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    auc = roc_auc_score(y_test, pred)\n",
    "    F1 = get_F1(precision,recall)\n",
    "    results = {\n",
    "        \"Loss\": loss,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"AUC\": auc,\n",
    "        \"F1 Score\": F1\n",
    "    }\n",
    "    # print(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_static_model = PassiveAggressiveClassifier(C = 0.5, random_state = RANDOM_STATE)\n",
    "\n",
    "pac_static_results = model_pipeline(pac_static_model, X_train_static_transformed, y_train, X_test_static_transformed, y_test)\n",
    "pa_static_results = pd.DataFrame(pac_static_results, index=[\"PA_static\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_dynamic_model = PassiveAggressiveClassifier(C = 0.5, random_state = RANDOM_STATE)\n",
    "\n",
    "pac_dynamic_results = model_pipeline(pac_dynamic_model, X_train_dynamic_transformed, y_train, X_test_dynamic_transformed, y_test)\n",
    "pa_dynamic_results = pd.DataFrame(pac_dynamic_results, index=[\"PA_dynamic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_static_model = LogisticRegression(random_state=RANDOM_STATE) \n",
    "\n",
    "logit_static_results = model_pipeline(logit_static_model, X_train_static_transformed, y_train, X_test_static_transformed, y_test)\n",
    "logit_static_results = pd.DataFrame(logit_static_results, index=[\"Logit_static\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_dynamic_model = LogisticRegression(random_state=RANDOM_STATE) \n",
    "\n",
    "logit_dynamic_results = model_pipeline(logit_dynamic_model, X_train_dynamic_transformed, y_train, X_test_dynamic_transformed, y_test)\n",
    "logit_dynamic_results = pd.DataFrame(logit_dynamic_results, index=[\"Logit_dynamic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_static_model = svm.SVC()\n",
    "svm_static_results = model_pipeline(svm_static_model, X_train_static_transformed, y_train, X_test_static_transformed, y_test)\n",
    "svm_static_results = pd.DataFrame(svm_static_results, index=[\"svm_static\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_dynamic_model = svm.SVC()\n",
    "svm_dynamic_results = model_pipeline(svm_dynamic_model, X_train_dynamic_transformed, y_train, X_test_dynamic_transformed, y_test)\n",
    "svm_dynamic_results = pd.DataFrame(svm_dynamic_results, index=[\"svm_dynamic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm_static_model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=RANDOM_STATE)\n",
    "lgbm_static_results = model_pipeline(lgbm_static_model, X_train_static_transformed, y_train, X_test_static_transformed, y_test)\n",
    "lgbm_static_results = pd.DataFrame(lgbm_static_results, index=[\"lgbm_static\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_dynamic_model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=RANDOM_STATE)\n",
    "lgbm_dynamic_results = model_pipeline(lgbm_dynamic_model, X_train_dynamic_transformed, y_train, X_test_dynamic_transformed, y_test)\n",
    "lgbm_dynamic_results = pd.DataFrame(lgbm_dynamic_results, index=[\"lgbm_dynamic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PA_static</th>\n",
       "      <td>13.700499</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.595678</td>\n",
       "      <td>0.525896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA_dynamic</th>\n",
       "      <td>14.736665</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.411348</td>\n",
       "      <td>0.564164</td>\n",
       "      <td>0.475410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logit_static</th>\n",
       "      <td>14.045932</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.567376</td>\n",
       "      <td>0.567376</td>\n",
       "      <td>0.591864</td>\n",
       "      <td>0.567376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logit_dynamic</th>\n",
       "      <td>13.815678</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.599759</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_static</th>\n",
       "      <td>13.700504</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.482270</td>\n",
       "      <td>0.596481</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_dynamic</th>\n",
       "      <td>12.664346</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.629198</td>\n",
       "      <td>0.589552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_static</th>\n",
       "      <td>13.470275</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.607989</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_dynamic</th>\n",
       "      <td>12.894634</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.626522</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Loss  Accuracy  Precision    Recall       AUC  F1 Score\n",
       "PA_static      13.700499  0.603333   0.600000  0.468085  0.595678  0.525896\n",
       "PA_dynamic     14.736665  0.573333   0.563107  0.411348  0.564164  0.475410\n",
       "Logit_static   14.045932  0.593333   0.567376  0.567376  0.591864  0.567376\n",
       "Logit_dynamic  13.815678  0.600000   0.571429  0.595745  0.599759  0.583333\n",
       "svm_static     13.700504  0.603333   0.596491  0.482270  0.596481  0.533333\n",
       "svm_dynamic    12.664346  0.633333   0.622047  0.560284  0.629198  0.589552\n",
       "lgbm_static    13.470275  0.610000   0.586957  0.574468  0.607989  0.580645\n",
       "lgbm_dynamic   12.894634  0.626667   0.598639  0.624113  0.626522  0.611111"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results = pd.concat([pa_static_results, pa_dynamic_results, logit_static_results,  logit_dynamic_results,svm_static_results,svm_dynamic_results, lgbm_static_results, lgbm_dynamic_results])\n",
    "combined_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f9a0a7ae4c556cc9d70b63c8d6e054a1e46c823d790d1334b23650b163d6b82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
